{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Адаптация подхода статьи \"[Automatic Scoring of English Essays Based on Machine Learning Technology in a Wireless Network Environment](https://www.hindawi.com/journals/scn/2022/9336298/#EEq5)\" к автопроверке письменной части ЕГЭ по английскому языку."
      ],
      "metadata": {
        "id": "Jt4Hgbs_ieqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "WLODSPra1qKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_data = pd.read_csv('email_data.csv')\n",
        "\n",
        "data_to_work = email_data.sample(n=300)"
      ],
      "metadata": {
        "id": "ZViB7_Tk1UpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.2. Linguistic Feature Extraction and Filtering"
      ],
      "metadata": {
        "id": "Nr65UGLPjQUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autocorrect\n",
        "!pip install language_tool_python\n",
        "!pip install textstat"
      ],
      "metadata": {
        "id": "Wf0odyNl1H0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "pepxsE_91NDP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b387ef-c450-41ce-bdea-5f7ee5e40652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-477XsS70hKc"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.util import ngrams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from textstat import syllable_count, flesch_reading_ease, flesch_kincaid_grade, gunning_fog\n",
        "from autocorrect import Speller\n",
        "import language_tool_python\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import word_tokenize\n",
        "\n",
        "spell = Speller(lang=\"en\")\n",
        "\n",
        "def lexical_chains(text):\n",
        "    chains = []\n",
        "    words = word_tokenize(text)\n",
        "    for word in words:\n",
        "        synsets = wn.synsets(word)\n",
        "        if synsets:\n",
        "            chain = set()\n",
        "            for synset in synsets:\n",
        "                chain.update(synset.lemma_names())\n",
        "            chains.append(chain)\n",
        "    return chains\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "\n",
        "# Очищаем текст от знаков препинания, цифр, стоп-слов и приводим к нижнему регистру\n",
        "# data_to_work[\"Text\"] = data_to_work[\"Text\"].str.replace(\"[^a-zA-Z]\", \" \") # Удаляем все, кроме букв\n",
        "# data_to_work[\"Text\"] = data_to_work[\"Text\"].str.lower() # Приводим к нижнему регистру\n",
        "# stop_words = stopwords.words(\"english\") # Загружаем список стоп-слов\n",
        "# data_to_work[\"Text\"] = data_to_work[\"Text\"].apply(lambda x: \" \".join([w for w in x.split() if w not in stop_words])) # Удаляем стоп-слова\n",
        "\n",
        "# Вычисляем поверхностные признаки на основе длины текста\n",
        "data_to_work[\"LEN_CHAR\"] = data_to_work[\"Text\"].apply(len) # Количество символов\n",
        "data_to_work[\"LEN_WORD\"] = data_to_work[\"Text\"].apply(lambda x: len(x.split())) # Количество слов\n",
        "data_to_work[\"LEN_PUNCT\"] = data_to_work[\"Text\"].apply(lambda x: len([c for c in x if c in \".,:;!?\"])) # Количество знаков препинания\n",
        "data_to_work[\"LEN_SENT\"] = data_to_work[\"Text\"].apply(lambda x: len(sent_tokenize(x))) # Количество предложений\n",
        "data_to_work[\"LEN_PARA\"] = data_to_work[\"Text\"].apply(lambda x: len(x.split(\"\\n\"))) # Количество абзацев\n",
        "data_to_work[\"LEN_AWL\"] = data_to_work[\"Text\"].apply(lambda x: np.mean([len(w) for w in x.split()])) # Средняя длина слова\n",
        "data_to_work[\"LEN_ASL\"] = data_to_work[\"Text\"].apply(lambda x: np.mean([len(s.split()) for s in sent_tokenize(x)])) # Средняя длина предложения\n",
        "\n",
        "# Вычисляем показатели читабельности текста\n",
        "data_to_work[\"RE_AWS\"] = data_to_work[\"Text\"].apply(lambda x: np.mean([syllable_count(w) for w in x.split()])) # Среднее количество слогов в слове\n",
        "data_to_work[\"RE_CWR\"] = data_to_work[\"Text\"].apply(lambda x: len([w for w in x.split() if syllable_count(w) > 2]) / len(x.split())) # Доля сложных слов (больше двух слогов)\n",
        "data_to_work[\"RE_FOG\"] = data_to_work[\"Text\"].apply(gunning_fog) # Индекс читабельности FOG\n",
        "data_to_work[\"RE_FLESCH\"] = data_to_work[\"Text\"].apply(flesch_reading_ease) # Индекс читабельности FLESCH\n",
        "data_to_work[\"RE_KINCAID\"] = data_to_work[\"Text\"].apply(flesch_kincaid_grade) # Индекс читабельности KINCAID\n",
        "\n",
        "# Вычисляем лексическое разнообразие текста\n",
        "data_to_work[\"TTR\"] = data_to_work[\"Text\"].apply(lambda x: len(set(x.split())) / len(x.split())) # Отношение количества разных слов к общему количеству слов\n",
        "data_to_work[\"TTR_ROOT\"] = data_to_work[\"Text\"].apply(lambda x: len(set(x.split())) / np.sqrt(len(x.split()))) # Корень из отношения количества разных слов к общему количеству слов\n",
        "data_to_work[\"TTR_LOG\"] = data_to_work[\"Text\"].apply(lambda x: np.log(len(set(x.split()))) / np.log(len(x.split()))) # Логарифм отношения количества разных слов к общему количеству слов\n",
        "data_to_work[\"TTR_SEG\"] = data_to_work[\"Text\"].apply(lambda x: np.mean([len(set(s.split())) / len(s.split()) for s in sent_tokenize(x)])) # Среднее отношение количества разных слов к общему количеству слов по предложениям\n",
        "data_to_work[\"TTR_RAND\"] = data_to_work[\"Text\"].apply(lambda x: np.mean([len(set(np.random.choice(x.split(), size=len(x.split()), replace=False))) / len(x.split()) for _ in range(10)])) # Среднее отношение количества разных слов к общему количеству слов по случайным выборкам\n",
        "\n",
        "# Вычисляем сложность синтаксиса текста\n",
        "data_to_work[\"pos\"] = data_to_work[\"Text\"].apply(lambda x: pos_tag(word_tokenize(x))) # Определяем часть речи для каждого слова в тексте\n",
        "data_to_work[\"SYN_C\"] = data_to_work[\"pos\"].apply(lambda x: len([w for w, t in x if t in [\"NN\", \"NNS\", \"NNP\", \"NNPS\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]])) # Количество синтаксических единиц (существительных и глаголов)\n",
        "data_to_work[\"SYN_DC\"] = data_to_work[\"pos\"].apply(lambda x: len([w for w, t in x if t in [\"WDT\", \"WP\", \"WP$\", \"WRB\", \"IN\"]])) # Количество подчинительных союзов\n",
        "data_to_work[\"SYN_VP\"] = data_to_work[\"pos\"].apply(lambda x: len([w for w, t in x if t.startswith(\"VB\")])) # Количество глагольных групп\n",
        "data_to_work[\"SYN_CN\"] = data_to_work[\"pos\"].apply(lambda x: len([w for w, t in x if t in [\"JJ\", \"JJR\", \"JJS\", \"DT\", \"PDT\", \"PRP$\", \"POS\", \"CD\"]])) # Количество сложных существительных групп\n",
        "data_to_work[\"SYN_CP\"] = data_to_work[\"pos\"].apply(lambda x: len([w for w, t in x if t in [\"CC\", \":\", \";\"]])) # Количество параллельных групп\n",
        "data_to_work[\"SYN_C\"] = data_to_work[\"SYN_C\"] / data_to_work[\"LEN_SENT\"] # Доля синтаксических единиц\n",
        "data_to_work[\"SYN_DC\"] = data_to_work[\"SYN_DC\"] / data_to_work[\"LEN_SENT\"] # Доля подчинительных союзов\n",
        "data_to_work[\"SYN_VP\"] = data_to_work[\"SYN_VP\"] / data_to_work[\"LEN_SENT\"] # Доля глагольных групп\n",
        "data_to_work[\"SYN_CN\"] = data_to_work[\"SYN_CN\"] / data_to_work[\"LEN_SENT\"] # Доля сложных существительных групп\n",
        "data_to_work[\"SYN_CP\"] = data_to_work[\"SYN_CP\"] / data_to_work[\"LEN_SENT\"] # Доля параллельных групп\n",
        "\n",
        "# Вычисляем правильность грамматики текста\n",
        "data_to_work[\"SPELL_E\"] = data_to_work[\"Text\"].apply(lambda x: len([w for w in x.split() if w != spell(w)])) # Количество орфографических ошибок\n",
        "data_to_work[\"GRM_E\"] = data_to_work[\"Text\"].apply(lambda x: len(tool.check(x))) # Количество сложных грамматических ошибок\n",
        "data_to_work[\"SPELL_E/W\"] = data_to_work[\"SPELL_E\"] / data_to_work[\"LEN_WORD\"] # Доля орфографических ошибок к общему количеству слов\n",
        "data_to_work[\"SPELL_E/S\"] = data_to_work[\"SPELL_E\"] / data_to_work[\"LEN_SENT\"] # Доля орфографических ошибок к общему количеству предложений\n",
        "data_to_work[\"GRM_E/W\"] = data_to_work[\"GRM_E\"] / data_to_work[\"LEN_WORD\"] # Доля сложных грамматических ошибок к общему количеству слов\n",
        "data_to_work[\"GRM_E/S\"] = data_to_work[\"GRM_E\"] / data_to_work[\"LEN_SENT\"] # Доля сложных грамматических ошибок к общему количеству предложений\n",
        "\n",
        "# Вычисляем связность дискурса текста\n",
        "data_to_work[\"Links_local\"] = data_to_work[\"Text\"].apply(lambda x: len([w for s in sent_tokenize(x) for w in ngrams(s.split(), 2) if w in lexical_chains(x)])) # Количество лексических связей между соседними предложениями\n",
        "data_to_work[\"Links_global\"] = data_to_work[\"Text\"].apply(lambda x: len([w for w in ngrams(x.split(), 2) if w in lexical_chains(x)])) # Количество лексических связей между любыми двумя предложениями\n",
        "data_to_work[\"COH_LOCAL\"] = data_to_work[\"Links_local\"] / data_to_work[\"LEN_SENT\"] # Доля лексических связей между соседними предложениями\n",
        "data_to_work[\"COH_GLOBAL\"] = data_to_work[\"Links_global\"] / data_to_work[\"LEN_SENT\"] # Доля лексических связей между любыми двумя предложениями"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_work.head()"
      ],
      "metadata": {
        "id": "VwjmgdhW0ovV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Фильтруем лингвистические признаки с помощью алгоритма Random Forest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=100, oob_score=True)\n",
        "X = data_to_work.drop([\" Type\", \"Question\", \"Text\", \"pos\", \"Overall_score\", \"Solving a communicative task\",\t\"Text structure\",\t\"Use of English (for emails)\"], axis=1)\n",
        "y = data_to_work[\"Overall_score\"]\n",
        "rf.fit(X, y)\n",
        "importance = rf.feature_importances_ # Получаем важность каждого признака\n",
        "X_sub = X.loc[:, importance > 0] # Выбираем признаки с важностью больше 0"
      ],
      "metadata": {
        "id": "LCqxZ9FL0rEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_sub)"
      ],
      "metadata": {
        "id": "bOFdQfEn1E2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_sub\n",
        "y = data_to_work[\"Overall_score\"].astype('float32') # баллы за эссе"
      ],
      "metadata": {
        "id": "fxHOEd3X14YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# линейная регрессия\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"RMSE: {np.sqrt(mse)}\")\n",
        "r_1 = np.corrcoef(y_test, y_pred)[0, 1] # коэффициент корреляции Пирсона для модели 1\n",
        "rho_1 = scipy.stats.spearmanr(y_test, y_pred)[0] # коэффициент корреляции Спирмена для модели 1\n",
        "print(\"r = {:.3f}, rho = {:.3f}\".format(r_1, rho_1))"
      ],
      "metadata": {
        "id": "XFxjOUx61veh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# дерево\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "model = DecisionTreeRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Предсказание на тестовой выборке и вычисление ошибки\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "print(f\"RMSE: {mse}\")"
      ],
      "metadata": {
        "id": "0xpR3dMs11JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# лес\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "print(f\"RMSE: {mse}\")"
      ],
      "metadata": {
        "id": "V6Zi2D8d197x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2.1. Bag-of-Words Feature Extraction and Filtering"
      ],
      "metadata": {
        "id": "orhcjlG7Azzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "v3Oj4CGrAnz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = email_data.sample(n=300)"
      ],
      "metadata": {
        "id": "1us_PJc8B2FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Очищаем текст от знаков препинания, цифр и приводим к нижнему регистру\n",
        "train_data[\"Text\"] = train_data[\"Text\"].str.replace(\"[^a-zA-Z]\", \" \", regex=True)\n",
        "train_data[\"Text\"] = train_data[\"Text\"].str.lower()"
      ],
      "metadata": {
        "id": "ELk9mpwMCDHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Разбиваем текст на отдельные слова и последовательности слов длиной от 1 до 3 (n-граммы)\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 3)) # Создаем объект класса CountVectorizer с параметром ngram_range\n",
        "X = vectorizer.fit_transform(train_data[\"Text\"]) # Преобразуем текст в матрицу частот слов и последовательностей\n",
        "vocab = vectorizer.get_feature_names_out() # Получаем список слов и последовательностей из словаря\n",
        "print(\"Размер словаря:\", len(vocab)) # Выводим размер словаря\n",
        "print(\"Первые 10 элементов словаря:\", vocab[:10]) # Выводим первые 10 элементов словаря\n",
        "\n",
        "# Определяем часть речи для каждого слова в тексте с помощью модуля nltk\n",
        "# nltk.download(\"punkt\") # Скачиваем модуль для токенизации\n",
        "# nltk.download(\"averaged_perceptron_tagger\") # Скачиваем модуль для определения части речи\n",
        "train_data[\"pos\"] = train_data[\"Text\"].apply(lambda x: nltk.pos_tag(nltk.word_tokenize(x))) # Добавляем новый столбец с кортежами вида (слово, тег)\n",
        "print(\"Первые 5 строк с частями речи:\")\n",
        "print(train_data[\"pos\"].head()) # Выводим первые 5 строк с частями речи\n",
        "\n",
        "# Сгруппировываем слова и последовательности по частям речи и подсчитываем частоту каждой группы\n",
        "pos_dict = {} # Создаем пустой словарь для хранения частот\n",
        "for row in train_data[\"pos\"]: # Для каждой строки в столбце pos\n",
        "    for word, tag in row: # Для каждого кортежа в строке\n",
        "        if tag not in pos_dict: # Если тега еще нет в словаре\n",
        "            pos_dict[tag] = 1 # Добавляем его с начальным значением 1\n",
        "        else: # Если тег уже есть в словаре\n",
        "            pos_dict[tag] += 1 # Увеличиваем его значение на 1\n",
        "print(\"Частота частей речи:\")\n",
        "print(pos_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Juz1VMgZC4zR",
        "outputId": "cc8614fb-a8f5-48eb-d02c-5f8045c842bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер словаря: 37114\n",
            "Первые 10 элементов словаря: ['10' '10 minutes' '10 minutes to' '100' '100 necessary'\n",
            " '100 necessary because' '11th' '11th floor' '11th floor which' '132']\n",
            "Первые 5 строк с частями речи:\n",
            "267    [(Dear, NNP), (Ralph, NNP), (,, ,), (Thanks, N...\n",
            "224    [(Dear, NNP), (,, ,), (Nora, NNP), (Thanks, VB...\n",
            "326    [(Dear, NNP), (Mark, NNP), (,, ,), (Thanks, NN...\n",
            "138    [(Dear, NNP), (Andy, NNP), (,, ,), (Thanks, NN...\n",
            "341    [(Dear, NNP), (Jane, NNP), (,, ,), (Thank, NNP...\n",
            "Name: pos, dtype: object\n",
            "Частота частей речи:\n",
            "{'NNP': 2093, ',': 2236, 'DT': 2413, 'NN': 5100, 'IN': 4659, 'PRP$': 1964, 'JJ': 2726, '.': 3810, 'PRP': 5008, 'VBP': 2305, 'RB': 2306, 'TO': 1279, 'VB': 2471, 'NNS': 2365, 'VBZ': 1218, 'VBD': 708, 'MD': 419, 'WRB': 394, 'VBG': 616, 'JJR': 220, 'WP': 381, 'JJS': 299, 'CC': 842, 'PDT': 39, 'RBR': 90, 'VBN': 408, 'WDT': 33, ':': 32, 'CD': 72, 'RP': 69, 'FW': 24, ')': 19, 'EX': 51, 'RBS': 27, 'POS': 101, '(': 18, 'UH': 33, '``': 55, 'NNPS': 5, \"''\": 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Вычисляем взаимную информацию для каждого слова и последовательности с помощью уравнения (3) из статьи\n",
        "mi_dict = {} # Создаем пустой словарь для хранения взаимной информации\n",
        "train_data[\"Overall_score\"] = train_data[\"Overall_score\"].astype(float)\n",
        "N = len(train_data) # Количество эссе в наборе данных\n",
        "m = float(train_data[\"Overall_score\"].median()) # Медиана оценок эссе\n",
        "D_high = train_data[train_data[\"Overall_score\"] >= m] # Подмножество эссе с высокой оценкой\n",
        "D_low = train_data[train_data[\"Overall_score\"] < m] # Подмножество эссе с низкой оценкой\n",
        "for v in vocab: # Для каждого слова или последовательности в словаре\n",
        "    n_11 = len(D_high[D_high[\"Text\"].str.contains(v)]) # Количество эссе с высокой оценкой, содержащих v\n",
        "    n_12 = len(D_low[D_low[\"Text\"].str.contains(v)]) # Количество эссе с низкой оценкой, содержащих v\n",
        "    n_21 = len(D_high) - n_11 # Количество эссе с высокой оценкой, не содержащих v\n",
        "    n_22 = len(D_low) - n_12 # Количество эссе с низкой оценкой, не содержащих v\n",
        "    n_1p = n_11 + n_12 # Количество эссе, содержащих v\n",
        "    n_p1 = n_11 + n_21 # Количество эссе с высокой оценкой\n",
        "    n_2p = n_21 + n_22 # Количество эссе, не содержащих v\n",
        "    n_p2 = n_12 + n_22 # Количество эссе с низкой оценкой\n",
        "    # Вычисляем взаимную информацию по формуле из статьи\n",
        "    mi_v = 0\n",
        "    if n_11 > 0:\n",
        "        mi_v += n_11 / N * np.log(N * n_11 / (n_1p * n_p1))\n",
        "    if n_21 > 0:\n",
        "        mi_v += n_21 / N * np.log(N * n_21 / (n_2p * n_p1))\n",
        "    if n_12 > 0:\n",
        "        mi_v += n_12 / N * np.log(N * n_12 / (n_1p * n_p2))\n",
        "    if n_22 > 0:\n",
        "        mi_v += n_22 / N * np.log(N * n_22 / (n_2p * n_p2))\n",
        "    mi_dict[v] = mi_v # Добавляем в словарь взаимную информацию для v\n",
        "print(\"Взаимная информация для слов и последовательностей:\")\n",
        "print(mi_dict) # Выводим словарь с взаимной информацией\n",
        "\n",
        "# Отфильтровываем слова и последовательности по длине и взаимной информации с помощью уравнения (1) из статьи и заданных пороговых значений\n",
        "t_len = 1 # Пороговое значение для длины\n",
        "t_mi = 0.01 # Пороговое значение для взаимной информации\n",
        "BOW_sub = [] # Создаем пустой список для отфильтрованных слов и последовательностей\n",
        "for v in vocab: # Для каждого слова или последовательности в словаре\n",
        "    len_v = len(v.split()) # Вычисляем длину v\n",
        "    mi_v = mi_dict[v] # Получаем взаимную информацию для v\n",
        "    if mi_v > t_mi: # Если v удовлетворяет условию фильтрации\n",
        "        BOW_sub.append(v) # Добавляем v в список\n",
        "print(\"Отфильтрованные слова и последовательности:\")\n",
        "print(BOW_sub) # Выводим список с отфильтрованными словами и последовательностями"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1Gszt-bDDZk",
        "outputId": "d62db8b0-7e0e-4e48-89fc-149e489eba57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Взаимная информация для слов и последовательностей:\n",
            "Отфильтрованные слова и последовательности:\n",
            "['about', 'about ecotourism', 'about friends', 'about my', 'about you', 'about your', 'about your gran', 'about your sister', 'ages', 'all for', 'always', 'always glad', 'always glad to', 'always help my', 'an exam', 'an online', 'and do the', 'animal', 'animals', 'answer', 'answer you', 'answer you earlier', 'are not as', 'as an', 'as good', 'as good as', 'as it', 'as my', 'as my brother', 'as my father', 'as my friend', 'as real', 'as real ones', 'as we', 'asked', 'asked me', 'asked me about', 'asking', 'asking me', 'asking me for', 'asking me to', 'asks', 'at online', 'author', 'back', 'back soon', 'beautiful', 'because it takes', 'biscuit', 'biscuits', 'biscuits made', 'by', 'camp', 'choose', 'chosen', 'circle', 'circle of', 'circle of friend', 'close friend', 'colour is it', 'could', 'couldn', 'days at', 'did it', 'direct', 'does it function', 'doing well', 'earlier', 'easy to', 'eat', 'eat that', 'ecotourism', 'email', 'father is', 'father is asking', 'for an', 'for an exam', 'for help', 'for not', 'for not writing', 'for so', 'for so long', 'for your message', 'for your recent', 'forest', 'forward', 'forward to', 'forward to hearing', 'friend is', 'friend is asking', 'friends', 'friends are', 'friends are not', 'friends at school', 'from', 'from you', 'function', 'garden', 'general', 'get message', 'get messages', 'get messages from', 'glad', 'glad to', 'glad to get', 'go as', 'go as my', 'good as', 'good as real', 'good university', 'got', 'got to', 'got to go', 'gran', 'grand', 'grandmother', 'grandpa', 'grandparents', 'grandparents live', 'granny', 'great', 'great that', 'great that you', 'guests will be', 'had', 'had to', 'had to prepare', 'hard', 'hard for', 'hard for me', 'harm', 'have', 'have grand', 'have grandparents', 'have to leave', 'have you', 'hearing', 'hearing from', 'hearing from you', 'help him', 'help him with', 'help the', 'help them', 'her is', 'him', 'him with', 'him with his', 'his car', 'his homework', 'hobby', 'il', 'il you', 'il you asked', 'in the village', 'is asking', 'is asking me', 'is great', 'is great to', 'is it', 'is its', 'it difficult', 'it function', 'it takes', 'leave', 'leave now', 'like it', 'line', 'lisa', 'little', 'live', 'live in', 'live in the', 'll', 'll be', 'made of', 'mail', 'mail you', 'mail you asked', 'make', 'make new', 'make new friends', 'me about', 'me about my', 'me do', 'me for', 'me for help', 'me more', 'me more about', 'me to', 'me to help', 'me to walk', 'media', 'mentioned', 'messages', 'messages from', 'messages from you', 'more', 'more about', 'more about you', 'more about your', 'morning', 'mostly', 'much time', 'my brother', 'my close', 'my father is', 'my friend is', 'my grand', 'my grandparents', 'natural', 'new friends', 'news', 'not as', 'not as good', 'not writing', 'not writing you', 'novel', 'of friend', 'of friends', 'off', 'offer', 'offers', 'often', 'often do', 'on holiday', 'ones because', 'online', 'online friend', 'online friends', 'online friends are', 'opportunities', 'opportunity', 'opportunity to', 'or help', 'or not', 'our school', 'out', 'out my', 'patient', 'people', 'people to', 'planning', 'popular', 'popular in', 'popular in my', 'prepare for', 'prepare for an', 'protect', 're doing', 're doing well', 're going to', 'read', 'real', 'real friends', 'real one', 'real ones', 'real ones because', 'recent', 'recent email', 'recipe', 'ring', 'rubbish out', 'see the', 'side', 'sister', 'so long', 'social', 'social media', 'sometimes', 'soon', 'sorry', 'sorry for', 'sorry for not', 'sport', 'take', 'take the rubbish', 'takes', 'tel', 'tell', 'tell me', 'tell me more', 'that on', 'that online', 'that online friends', 'that you', 'the exam', 'the exams', 'the result', 'the rubbish', 'the rubbish out', 'the way', 'think it', 'think it is', 'think that online', 'think that the', 'time do', 'time for', 'time for me', 'to choose', 'to get', 'to get message', 'to get messages', 'to go as', 'to hearing', 'to hearing from', 'to help him', 'to leave', 'to leave now', 'to prepare', 'to prepare for', 'to walk', 'to walk my', 'tourism', 'tourism is', 'traditional', 'try to help', 've got', 've got to', 've mentioned', 'very important', 'very interesting', 'via social', 'via social media', 'walk my', 'walk my dog', 'ways', 'wedding', 'were the', 'will', 'will be', 'with his', 'with his car', 'with his homework', 'would like', 'would like to', 'writing', 'writing you', 'writing you for', 'you as', 'you ask', 'you asked', 'you asked me', 'you earlier', 'you for so', 'you had', 'you like', 'your gran', 'your grand', 'your grandmother', 'your granny', 'your message', 'your recent', 'your recent email', 'your school', 'your sister', 'youth', 'youth camp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Взвешиваем частоты слов с помощью бинарного или TF-IDF метода с помощью уравнений (4) и (5) из статьи\n",
        "method = \"tfidf\" # Выбираем метод взвешивания\n",
        "if method == \"binary\": # Если метод бинарный\n",
        "    def binary(c): # Определяем функцию бинарного взвешивания\n",
        "        if c >= 1: # Если частота больше или равна 1\n",
        "            return 1 # Возвращаем 1\n",
        "        else: # Иначе\n",
        "            return 0 # Возвращаем 0\n",
        "    X_sub = X[:, [vectorizer.vocabulary_.get(v) for v in BOW_sub]] # Выбираем столбцы матрицы X, соответствующие отфильтрованным словам и последовательностям\n",
        "    X_sub = X_sub.toarray() # Преобразуем матрицу в массив numpy\n",
        "    binary = np.vectorize(binary, otypes=[int]) # Векторизуем функцию бинарного взвешивания\n",
        "    X_sub = binary(X_sub) # Применяем функцию бинарного взвешивания к каждому элементу массива\n",
        "elif method == \"tfidf\": # Если метод TF-IDF\n",
        "    df_v = X[:, [vectorizer.vocabulary_.get(v) for v in BOW_sub]].sum(axis=0) # Считаем количество документов, содержащих каждое слово или последовательность\n",
        "    def tfidf(c, v): # Определяем функцию TF-IDF взвешивания\n",
        "        return (1 + np.log(c)) * np.log(N / df_v[v]) # Возвращаем произведение логарифма частоты и обратной частоты документа\n",
        "    X_sub = X[:, [vectorizer.vocabulary_.get(v) for v in BOW_sub]] # Выбираем столбцы матрицы X, соответствующие отфильтрованным словам и последовательностям\n",
        "    X_sub = X_sub.toarray() # Преобразуем матрицу в массив numpy\n",
        "\n",
        "\n",
        "y = train_data[\"Overall_score\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sub, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Обучаем модель SVR на обучающей выборке и предсказываем оценки для валидационной выборки\n",
        "svr = SVR()\n",
        "svr.fit(X_train, y_train)\n",
        "y_pred = svr.predict(X_test)"
      ],
      "metadata": {
        "id": "0gOa-B0JDGfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2^:\", r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFZrDWPWDMaG",
        "outputId": "79c2741b-21c8-4668-d107-4c60d0d69c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 1.2018101895388804\n",
            "MAE: 0.7542043335556252\n",
            "R^2^: 0.09011215933964878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Experiments and Results"
      ],
      "metadata": {
        "id": "xCUhbMo_29h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "email_data = pd.read_csv('email_data.csv')\n",
        "\n",
        "# Очищаем текст от знаков препинания, цифр, стоп-слов и приводим к нижнему регистру\n",
        "email_data[\"words\"] = email_data[\"Text\"].str.replace(\"[^a-zA-Z]\", \" \", regex=True) # Удаляем все, кроме букв\n",
        "email_data[\"words\"] = email_data[\"words\"].str.lower() # Приводим к нижнему регистру\n",
        "email_data[\"pos\"] = email_data[\"words\"].apply(lambda x: nltk.pos_tag(nltk.word_tokenize(x))) # Добавляем новый столбец с кортежами вида (слово, тег)\n",
        "email_data[\"Overall_score\"] = email_data[\"Overall_score\"].astype('float32')"
      ],
      "metadata": {
        "id": "1BizxWEL3MfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Функция для подсчета частей речи\n",
        "def count_pos(pos_list):\n",
        "    # Извлекаем только теги частей речи\n",
        "    tags = [tag for word, tag in pos_list]\n",
        "    # Подсчитываем количество каждой части речи\n",
        "    tag_counts = Counter(tags)\n",
        "    # Преобразуем словарь в строку, где каждая пара \"тег-количество\" разделена пробелом\n",
        "    tag_counts_str = ' '.join([f'{tag}:{count}' for tag, count in tag_counts.items()])\n",
        "    return tag_counts_str\n",
        "\n",
        "# Применяем функцию к столбцу 'pos'\n",
        "email_data['pos'] = email_data['pos'].apply(count_pos)"
      ],
      "metadata": {
        "id": "oZUbiIfz3XhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.sparse import hstack\n",
        "import scipy.stats\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "data = email_data\n",
        "\n",
        "train, val = sklearn.model_selection.train_test_split(data, test_size=0.1, random_state=42)\n",
        "C = 1\n",
        "epsilon = 0.1\n",
        "\n",
        "# Создаем векторизаторы для слов и частей речи\n",
        "word_vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=N)\n",
        "pos_vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=N)\n",
        "\n",
        "# Обучаем векторизаторы на обучающей выборке\n",
        "word_vectorizer.fit(train[\"words\"])\n",
        "pos_vectorizer.fit(train[\"pos\"])\n",
        "\n",
        "# Преобразуем обучающую и валидационную выборки в векторы\n",
        "X_train_words = word_vectorizer.transform(train[\"words\"])\n",
        "X_train_pos = pos_vectorizer.transform(train[\"pos\"])\n",
        "X_val_words = word_vectorizer.transform(val[\"words\"])\n",
        "X_val_pos = pos_vectorizer.transform(val[\"pos\"])\n",
        "\n",
        "# Объединяем векторы слов и частей речи в один признаковый вектор\n",
        "X_train = hstack((X_train_words, X_train_pos))\n",
        "X_val = hstack((X_val_words, X_val_pos))\n",
        "\n",
        "# Создаем модель линейной регрессии с эпсилон-чувствительной потерей\n",
        "model = LinearSVR(C=C, epsilon=epsilon, random_state=42)\n",
        "model.fit(X_train, train[\"Overall_score\"])\n",
        "val_predictions = model.predict(X_val)\n",
        "\n",
        "r_2 = np.corrcoef(val[\"Overall_score\"], val_predictions)[0, 1]\n",
        "rho_2 = scipy.stats.spearmanr(val[\"Overall_score\"], val_predictions)[0]\n",
        "rmse_2 = np.sqrt(np.mean((val[\"Overall_score\"] - val_predictions) ** 2))\n",
        "mse = mean_squared_error(val[\"Overall_score\"], val_predictions)\n",
        "mae = mean_absolute_error(val[\"Overall_score\"], val_predictions)\n",
        "r2 = r2_score(val[\"Overall_score\"], val_predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2^:\", r2)\n",
        "print(\"Model 2: r = {:.3f}, rho = {:.3f}, rmse = {:.3f}\".format(r_2, rho_2, rmse_2))"
      ],
      "metadata": {
        "id": "6VGvA6IT3b9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "7kUJjG-F4G_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Deep Learning-Based Model Building and Evaluation"
      ],
      "metadata": {
        "id": "Mr3cgaXuH1Sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import sklearn\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Flatten\n",
        "import scipy.stats"
      ],
      "metadata": {
        "id": "5bjZiikVIHll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_to_work[\"Text\"] # тексты эссе\n",
        "y = data_to_work[\"Overall_score\"].astype('float32') # баллы за эссе\n",
        "\n",
        "# Разбиваем данные на обучающую и валидационную выборки\n",
        "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X, y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "istseR3DIFrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# В общем опять. Можно было не делать, но что б ничего не рушить..\n",
        "\n",
        "# Создаем словарь из слов и частей речи\n",
        "vocab = {}\n",
        "for essay in X_train:\n",
        "  # Разбиваем эссе на предложения\n",
        "  sentences = nltk.sent_tokenize(essay)\n",
        "  for sentence in sentences:\n",
        "    # Разбиваем предложение на слова\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    # Определяем части речи для слов\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    # Добавляем слова и части речи в словарь с их частотами\n",
        "    for word, tag in pos_tags:\n",
        "      vocab[word] = vocab.get(word, 0) + 1\n",
        "      vocab[tag] = vocab.get(tag, 0) + 1\n",
        "\n",
        "# Вычисляем размер словаря\n",
        "vocab_size = len(vocab.keys())\n",
        "\n",
        "# Вычисляем размер словаря частей речи\n",
        "pos_size = 0\n",
        "for key in vocab.keys():\n",
        "  if key.isupper(): # предположим, что части речи записаны заглавными буквами\n",
        "    pos_size += 1"
      ],
      "metadata": {
        "id": "UZRM6KywLPZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем токенизатор и обучаем на текстах\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Преобразуем тексты в последовательности числовых идентификаторов\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "# Делаем дополнение или обрезку последовательностей, чтобы они все имели одинаковую длину\n",
        "X_train_seq_padded = pad_sequences(X_train_seq, maxlen=900)\n",
        "X_val_seq_padded = pad_sequences(X_val_seq, maxlen=900)"
      ],
      "metadata": {
        "id": "HWTti29ELG8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем словарь из частей речи и их индексов\n",
        "pos_dict = {}\n",
        "pos_index = 1\n",
        "for essay in X_train:\n",
        "  sentences = nltk.sent_tokenize(essay)\n",
        "  for sentence in sentences:\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    for word, tag in pos_tags:\n",
        "      if tag not in pos_dict:\n",
        "        pos_dict[tag] = pos_index\n",
        "        pos_index += 1\n",
        "\n",
        "# Преобразуем обучающую выборку в последовательности индексов частей речи\n",
        "X_train_pos = []\n",
        "for essay in X_train:\n",
        "  pos_seq = []\n",
        "  sentences = nltk.sent_tokenize(essay)\n",
        "  for sentence in sentences:\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    for word, tag in pos_tags:\n",
        "      pos_seq.append(pos_dict[tag])\n",
        "  X_train_pos.append(pos_seq)\n",
        "\n",
        "# Делаем то же самое с валидационной выборкой\n",
        "X_val_pos = []\n",
        "for essay in X_val:\n",
        "  pos_seq = []\n",
        "  sentences = nltk.sent_tokenize(essay)\n",
        "  for sentence in sentences:\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    for word, tag in pos_tags:\n",
        "      pos_seq.append(pos_dict[tag])\n",
        "  X_val_pos.append(pos_seq)\n",
        "\n",
        "# Это можно было объединить, но как есть."
      ],
      "metadata": {
        "id": "-78t5WsSIa6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Дополняем последовательности до максимальной длины, иначе дальше с размерностями будут сложности\n",
        "\n",
        "X_train_pos = pad_sequences(X_train_pos, maxlen=900, padding='post')\n",
        "X_val_pos = pad_sequences(X_val_pos, maxlen=900, padding='post')\n",
        "X_train_seq_padded = pad_sequences(X_train_seq_padded, maxlen=900, padding='post')\n",
        "X_train_pos = pad_sequences(X_train_pos, maxlen=900, padding='post')"
      ],
      "metadata": {
        "id": "CG_5DDC9I1wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Задаем параметры моделей\n",
        "\n",
        "dinput_length = 900 # длина словесной последовательности во входном слое\n",
        "dword_embedding = 100 # размерность вектора слов\n",
        "dpos_embedding = 50 # размерность вектора частей речи\n",
        "h = 20 # количество фильтров в сверточном слое\n",
        "m = 1 # длина окна свертки\n",
        "n = 2 # длина окна максимального пулинга\n",
        "n_model2 = 1 # длина окна максимального пулинга\n",
        "ddense = 128 # размерность полносвязного слоя\n",
        "batch_size = 16 # размер пакета для оптимизатора"
      ],
      "metadata": {
        "id": "BvxYjtvDJ9pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем модель 1, которая использует только словесные последовательности\n",
        "\n",
        "model_1 = tf.keras.Sequential()\n",
        "# Добавляем слой Word2vec, который преобразует слова в векторы\n",
        "model_1.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=dword_embedding, input_length=dinput_length ))\n",
        "# Добавляем сверточный слой, который применяет фильтры к входному слою\n",
        "model_1.add(tf.keras.layers.Conv1D(filters=h, kernel_size=m, activation='relu'))\n",
        "# Добавляем слой максимального пулинга, который уменьшает размерность и выбирает наиболее значимые признаки\n",
        "model_1.add(tf.keras.layers.MaxPooling1D(pool_size=n))\n",
        "# Добавляем полносвязный слой, который преобразует выход слоя максимального пулинга в вектор\n",
        "model_1.add(tf.keras.layers.Dense(units=ddense, activation='relu'))\n",
        "# Добавляем слой, который дает нам среднее значение\n",
        "model_1.add(Flatten())\n",
        "# Добавляем выходной слой, который предсказывает оценку эссе\n",
        "model_1.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
        "\n",
        "# Компилируем модели с оптимизатором Adam и функцией потерь среднеквадратичной ошибки\n",
        "model_1.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Обучаем модели на обучающей выборке и проверяем их на валидационной выборке\n",
        "model_1.fit(X_train_seq_padded, y_train, batch_size=batch_size, epochs=10, validation_data=(X_val_seq_padded, y_val))\n",
        "\n",
        "\n",
        "y_pred_1 = np.round(model_1.predict(X_val_seq_padded).flatten())\n",
        "\n",
        "r_1 = np.corrcoef(y_val, y_pred_1)[0, 1]\n",
        "rho_1 = scipy.stats.spearmanr(y_val, y_pred_1)[0]\n",
        "rmse_1 = np.sqrt(np.mean((y_val - y_pred_1) ** 2))\n",
        "mse = mean_squared_error(y_val, y_pred_1)\n",
        "mae = mean_absolute_error(y_val, y_pred_1)\n",
        "r2 = r2_score(y_val, y_pred_1)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2^:\", r2)\n",
        "print(\"Model 1: r = {:.3f}, rho = {:.3f}, rmse = {:.3f}\".format(r_1, rho_1, rmse_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCWwgTAcKB7j",
        "outputId": "e7a68a11-e275-45a5-93e0-c6e400e3be49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "17/17 [==============================] - 1s 37ms/step - loss: 7.9577 - val_loss: 2.7459\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 0s 27ms/step - loss: 1.6606 - val_loss: 1.0795\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 0s 27ms/step - loss: 1.1301 - val_loss: 1.0235\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 0s 25ms/step - loss: 0.8183 - val_loss: 0.9879\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 0s 26ms/step - loss: 0.7294 - val_loss: 0.9419\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 0s 25ms/step - loss: 0.6175 - val_loss: 0.8963\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 0s 27ms/step - loss: 0.4904 - val_loss: 0.8817\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 0s 26ms/step - loss: 0.3864 - val_loss: 0.9253\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 0s 27ms/step - loss: 0.2755 - val_loss: 0.8548\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 0s 25ms/step - loss: 0.1664 - val_loss: 0.8695\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "MSE: 0.8333333\n",
            "MAE: 0.56666666\n",
            "R^2^: 0.22600620223412082\n",
            "Model 1: r = 0.487, rho = 0.529, rmse = 0.913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dinput_length_model2 = 900 # длина словесной последовательности во входном слое\n",
        "\n",
        "# Создаем модель 2, которая использует словесные и частеречные последовательности\n",
        "model_2 = tf.keras.Sequential()\n",
        "# Добавляем слой Word2vec, который преобразует слова в векторы\n",
        "model_2.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=dword_embedding, input_length=dinput_length_model2))\n",
        "# Добавляем сверточный слой, который применяет фильтры к входному слою\n",
        "model_2.add(tf.keras.layers.Conv1D(filters=h, kernel_size=m, activation='relu'))\n",
        "# Добавляем слой максимального пулинга, который уменьшает размерность и выбирает наиболее значимые признаки\n",
        "model_2.add(tf.keras.layers.MaxPooling1D(pool_size=n_model2))\n",
        "# Добавляем слой глобального максимального пулинга\n",
        "model_2.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "# Добавляем полносвязный слой, который преобразует выход слоя максимального пулинга в вектор\n",
        "model_2.add(tf.keras.layers.Dense(units=ddense, activation='relu'))\n",
        "\n",
        "# Создаем еще одну модель, которая использует частеречные последовательности\n",
        "model_pos = tf.keras.Sequential()\n",
        "# Добавляем слой, который преобразует части речи в векторы\n",
        "model_pos.add(tf.keras.layers.Embedding(input_dim=pos_size, output_dim=dpos_embedding, input_length=dinput_length_model2))\n",
        "# Добавляем сверточный слой, который применяет фильтры к входному слою\n",
        "model_pos.add(tf.keras.layers.Conv1D(filters=h, kernel_size=m, activation='relu'))\n",
        "# Добавляем слой максимального пулинга, который уменьшает размерность и выбирает наиболее значимые признаки\n",
        "model_pos.add(tf.keras.layers.MaxPooling1D(pool_size=n))\n",
        "# Добавляем слой глобального максимального пулинга\n",
        "model_pos.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "# Объединяем выходы двух моделей в один слой слияния\n",
        "merged = tf.keras.layers.concatenate([model_2.output, model_pos.output])\n",
        "# Добавляем выходной слой, который предсказывает оценку эссе\n",
        "output = tf.keras.layers.Dense(units=1, activation='linear')(merged)\n",
        "# Создаем итоговую модель 2, которая использует обе последовательности\n",
        "model_2 = tf.keras.Model(inputs=[model_2.input, model_pos.input], outputs=output)\n",
        "\n",
        "model_2.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_2.fit([X_train_seq_padded, X_train_pos], y_train, batch_size=batch_size, epochs=10, validation_data=([X_val_seq_padded, X_val_pos], y_val))\n",
        "\n",
        "y_pred_2 = model_2.predict([X_val_seq_padded, X_val_pos]).flatten()\n",
        "\n",
        "r_2 = np.corrcoef(y_val, y_pred_2)[0, 1]\n",
        "rho_2 = scipy.stats.spearmanr(y_val, y_pred_2)[0]\n",
        "rmse_2 = np.sqrt(np.mean((y_val - y_pred_2) ** 2))\n",
        "mse = mean_squared_error(y_val, y_pred_2)\n",
        "mae = mean_absolute_error(y_val, y_pred_2)\n",
        "r2 = r2_score(y_val, y_pred_2)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2^:\", r2)\n",
        "print(\"Model 2: r = {:.3f}, rho = {:.3f}, rmse = {:.3f}\".format(r_2, rho_2, rmse_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeYpU49AKIWb",
        "outputId": "2ca78f5b-ba80-45c3-cee2-0a6e5e82aade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "17/17 [==============================] - 2s 41ms/step - loss: 24.0914 - val_loss: 19.2585\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 1s 50ms/step - loss: 11.7594 - val_loss: 4.0822\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 1s 51ms/step - loss: 1.8675 - val_loss: 1.6489\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 1s 50ms/step - loss: 1.3078 - val_loss: 1.1269\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 1s 37ms/step - loss: 0.9638 - val_loss: 1.0060\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 1s 32ms/step - loss: 0.8892 - val_loss: 0.9748\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 1s 31ms/step - loss: 0.8493 - val_loss: 0.9430\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 1s 33ms/step - loss: 0.8014 - val_loss: 0.9209\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 1s 31ms/step - loss: 0.7522 - val_loss: 0.8982\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 1s 30ms/step - loss: 0.6885 - val_loss: 0.8975\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "MSE: 0.8974987\n",
            "MAE: 0.7227685\n",
            "R^2^: 0.1664099552300512\n",
            "Model 2: r = 0.436, rho = 0.503, rmse = 0.947\n"
          ]
        }
      ]
    }
  ]
}
