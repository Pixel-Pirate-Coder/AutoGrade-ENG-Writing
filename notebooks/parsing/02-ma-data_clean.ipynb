{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Чтение и грубая очистка данных"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8542cad240673e9e"
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, re\n",
    "from collections import Counter\n",
    "\n",
    "from src.data.read_raw_data import read_from_gsheet, drop_unwanted_data, split_types"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T19:33:07.697578200Z",
     "start_time": "2023-11-25T19:33:07.687545200Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\max_a\\AppData\\Local\\Temp\\ipykernel_2956\\1533711228.py:2: DeprecationWarning: [Deprecated][in version 6.0.0]: client_factory will be replaced by gspread.http_client types\n",
      "  raw_data = read_from_gsheet()\n"
     ]
    }
   ],
   "source": [
    "# Читаем данные из таблицы, удаляем неинтересующие столбцы, фильтруем по типу задания\n",
    "raw_data = read_from_gsheet()\n",
    "_, email_data = split_types(raw_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T19:34:25.230949700Z",
     "start_time": "2023-11-25T19:34:22.063295700Z"
    }
   },
   "id": "2bd174dcb542409b"
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "data": {
      "text/plain": "    Type Question id                                           Question  \\\n2  Email              …I am so happy that summer has come and we are...   \n3  Email              …I am learning to cook from my mother now. But...   \n4  Email              …I am so happy that summer has come and we are...   \n5  Email              …I’ve recently been involved in a school surve...   \n6  Email              …All of my friends think camping is a perfect ...   \n\n                                                Text  \\\n2  Dear Ronny, I was glad to hear from you again....   \n3  Dear Mary, Thanks for your message. It was gre...   \n4  Moscow\\n15 october\\nHi!\\nThanks you for you re...   \n5  Hey, Mike.\\r\\nHow's it going? As for me, i'm p...   \n6  Hi Emily,\\r\\nThank you for the e-mail. I'm so ...   \n\n  Solving a communicative task Text structure Use of English (for emails)  \\\n2                            1              2                           2   \n3                            1              2                           2   \n4                            1              0                           0   \n5                            0              0                           0   \n6                            2              1                           0   \n\n                                            Comments Overall_score origin  \\\n2  К1 – 1 (отсутствует третий аспект). Лексико-гр...             5   ФИПИ   \n3  По критерию «Решение коммуникативной задачи»: ...             5   ФИПИ   \n4  По ОТ допущены следующие нарушения.\\r\\n1. Указ...             1   ФИПИ   \n5  Подведём итог по критерию «Решение коммуникати...             0   ФИПИ   \n6  Подведём итог по критерию «Решение коммуникати...             3   ФИПИ   \n\n  task_image score_image  \n2                         \n3                         \n4                         \n5                         \n6                         ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>Question id</th>\n      <th>Question</th>\n      <th>Text</th>\n      <th>Solving a communicative task</th>\n      <th>Text structure</th>\n      <th>Use of English (for emails)</th>\n      <th>Comments</th>\n      <th>Overall_score</th>\n      <th>origin</th>\n      <th>task_image</th>\n      <th>score_image</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>Email</td>\n      <td></td>\n      <td>…I am so happy that summer has come and we are...</td>\n      <td>Dear Ronny, I was glad to hear from you again....</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>К1 – 1 (отсутствует третий аспект). Лексико-гр...</td>\n      <td>5</td>\n      <td>ФИПИ</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Email</td>\n      <td></td>\n      <td>…I am learning to cook from my mother now. But...</td>\n      <td>Dear Mary, Thanks for your message. It was gre...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>По критерию «Решение коммуникативной задачи»: ...</td>\n      <td>5</td>\n      <td>ФИПИ</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Email</td>\n      <td></td>\n      <td>…I am so happy that summer has come and we are...</td>\n      <td>Moscow\\n15 october\\nHi!\\nThanks you for you re...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>По ОТ допущены следующие нарушения.\\r\\n1. Указ...</td>\n      <td>1</td>\n      <td>ФИПИ</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Email</td>\n      <td></td>\n      <td>…I’ve recently been involved in a school surve...</td>\n      <td>Hey, Mike.\\r\\nHow's it going? As for me, i'm p...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Подведём итог по критерию «Решение коммуникати...</td>\n      <td>0</td>\n      <td>ФИПИ</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Email</td>\n      <td></td>\n      <td>…All of my friends think camping is a perfect ...</td>\n      <td>Hi Emily,\\r\\nThank you for the e-mail. I'm so ...</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Подведём итог по критерию «Решение коммуникати...</td>\n      <td>3</td>\n      <td>ФИПИ</td>\n      <td></td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Далее будем работать с данными по письмам\n",
    "email_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T19:34:25.970023900Z",
     "start_time": "2023-11-25T19:34:25.964681500Z"
    }
   },
   "id": "ba14f04daa09bb81"
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "email_data = email_data[email_data['Text'].str.strip().astype(bool)] # Удалим строки с пустыми ответами"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T19:34:31.956412400Z",
     "start_time": "2023-11-25T19:34:31.942174800Z"
    }
   },
   "id": "4a24337909a94170"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Очистка и токенизация текста"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd42395415958852"
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\max_a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\max_a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import contractions\n",
    "\n",
    "from collections import Counter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T19:34:41.280659400Z",
     "start_time": "2023-11-25T19:34:40.993069100Z"
    }
   },
   "id": "f84dbef2427a71c2"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "email_data['Text'] = email_data['Text'].str.lower() # Приведем символы к нижнему регистру\n",
    "email_data['Text'] = email_data['Text'].apply(lambda x: contractions.fix(x)) # Преобразуем сокращения в полную запись: e.g. I'll -> I will\n",
    "email_data['Text'] = email_data['Text'].apply(lambda x: re.sub('[\\.\\n\\r\\t]', ' ', x)) # Уберем символы переноса строк, табуляции\n",
    "email_data['Text'] = email_data['Text'].apply(lambda x: re.sub('[А-Яа-я]', ' ', x))  # Уберем кириллицу\n",
    "email_data['Text'] = email_data['Text'].apply(lambda x: re.sub(r'[\\]!\"$%&\\'()*+,./:;=#@?\\[\\\\^_`{|}~-’\\d<>]+', \" \", x)) # Удалим символы пунктуации"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T18:30:41.287881600Z",
     "start_time": "2023-11-25T18:30:41.243178400Z"
    }
   },
   "id": "37803591ac1b1916"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "# Добавим колонку с токенизированным текстом (показалось, что правильнее будет токенизировать тексты по отдельности)\n",
    "email_data['tokenized'] = email_data['Text'].apply(word_tokenize)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T18:30:41.740457500Z",
     "start_time": "2023-11-25T18:30:41.584870900Z"
    }
   },
   "id": "bd1b0e1c96613a08"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "# Возможно, НЕ стоит очищать текст от stopwords, так как при этом, например, мы теряем конструкции со вспомогательным глаголом\n",
    "# filtered_tokens = [word for word in tokens if not word in stopwords.words('english')]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T18:30:41.960873700Z",
     "start_time": "2023-11-25T18:30:41.949373100Z"
    }
   },
   "id": "55964d5146296323"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-граммы\n",
      "25 самых популярных\n",
      "(('i',), 2538) (('to',), 1775) (('you',), 1439) (('the',), 1393) (('your',), 1302) (('is',), 1172) (('me',), 1108) (('my',), 1050) (('for',), 1050) (('a',), 966) (('in',), 899) (('and',), 815) (('about',), 789) (('it',), 766) (('have',), 756) (('of',), 535) (('that',), 505) (('with',), 485) (('am',), 479) (('what',), 466) (('from',), 451) (('email',), 416) (('dear',), 391) (('best',), 387) (('are',), 381)\n",
      "Всего 54557\n",
      "\n",
      "2-граммы\n",
      "25 самых популярных\n",
      "(('i', 'am'), 478) (('i', 'have'), 446) (('for', 'your'), 415) (('from', 'you'), 363) (('best', 'wishes'), 338) (('in', 'your'), 325) (('me', 'about'), 317) (('the', 'way'), 312) (('by', 'the'), 307) (('asked', 'me'), 293) (('you', 'asked'), 281) (('it', 'is'), 275) (('tell', 'me'), 255) (('more', 'about'), 254) (('about', 'your'), 247) (('way', 'tell'), 245) (('your', 'email'), 240) (('your', 'recent'), 234) (('me', 'more'), 233) (('you', 'in'), 226) (('to', 'get'), 225) (('am', 'always'), 210) (('that', 'is'), 198) (('messages', 'from'), 193) (('email', 'i'), 189)\n",
      "Всего 54556\n",
      "\n",
      "3-граммы\n",
      "25 самых популярных\n",
      "(('by', 'the', 'way'), 306) (('you', 'asked', 'me'), 275) (('asked', 'me', 'about'), 260) (('the', 'way', 'tell'), 245) (('way', 'tell', 'me'), 244) (('tell', 'me', 'more'), 232) (('me', 'more', 'about'), 231) (('for', 'your', 'recent'), 229) (('from', 'you', 'in'), 225) (('you', 'in', 'your'), 222) (('i', 'am', 'always'), 210) (('messages', 'from', 'you'), 193) (('more', 'about', 'your'), 192) (('to', 'get', 'messages'), 187) (('get', 'messages', 'from'), 187) (('thanks', 'for', 'your'), 182) (('in', 'your', 'email'), 181) (('your', 'email', 'you'), 176) (('drop', 'me', 'a'), 175) (('me', 'a', 'line'), 175) (('all', 'for', 'now'), 173) (('that', 'is', 'all'), 165) (('is', 'all', 'for'), 163) (('your', 'recent', 'email'), 158) (('email', 'you', 'asked'), 156)\n",
      "Всего 54555\n"
     ]
    }
   ],
   "source": [
    "tokens = list()\n",
    "for tokens_list in email_data['tokenized']:\n",
    "    tokens.extend(tokens_list)\n",
    "\n",
    "for i in range(1,4):\n",
    "    print(f'{i}-граммы')\n",
    "    igrams = ngrams(tokens, i)\n",
    "    icnt = Counter(igrams)\n",
    "    n = 25\n",
    "    print(f'{n} самых популярных')\n",
    "    print(*icnt.most_common(25))\n",
    "    print(f'Всего {icnt.total()}\\n')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T18:30:42.999842Z",
     "start_time": "2023-11-25T18:30:42.982915700Z"
    }
   },
   "id": "6ea82aaeab0acf16"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "# Соберем токены обратно в текст\n",
    "email_data['prep_text'] = [' '.join(map(str, l)) for l in email_data['tokenized']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T18:30:46.714455400Z",
     "start_time": "2023-11-25T18:30:46.685793700Z"
    }
   },
   "id": "c67a00776302d392"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Кодирование текста"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb72a9507a73d6d9"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T18:20:29.047581100Z",
     "start_time": "2023-11-25T18:20:29.039571800Z"
    }
   },
   "id": "6b1fc759861470cc"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "features = ['Question', 'prep_text']\n",
    "targets = ['Solving a communicative task', 'Text structure', 'Use of English (for emails)']\n",
    "\n",
    "test_size = int(0.25 * email_data.shape[0])  # Количество документов в тестовой выборке\n",
    "\n",
    "# Разделим на тестовые и тренировочные данные\n",
    "test_data_index = email_data.sample(test_size).index\n",
    "train_data_index = email_data.index.difference(test_data_index)\n",
    "test_data = email_data.loc[test_data_index]\n",
    "train_data = email_data.loc[train_data_index]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T18:29:03.742261100Z",
     "start_time": "2023-11-25T18:29:03.733548600Z"
    }
   },
   "id": "e214b613b0820867"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "def vectorize_as_bongrams(texts: list[str], ngram_min=1, ngram_max=3, max_words=12000):\n",
    "    \"\"\"\n",
    "    Кодирует список текстов в список из закодированных векторов с помощью Bag of N-grams\n",
    "    :param texts: Список текстов для кодирования\n",
    "    :param ngram_min: Минимальная кратность n-граммы\n",
    "    :param ngram_max: Максимальная кратность n-граммы\n",
    "    :param max_words: Максимальное количество слов в словаре (размерность признакового пространства)\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(ngram_min, ngram_max), max_features=max_words)\n",
    "    emb = count_vectorizer.fit_transform(texts).toarray()\n",
    "    return emb, count_vectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T18:30:52.796769100Z",
     "start_time": "2023-11-25T18:30:52.775251600Z"
    }
   },
   "id": "26bac00f6a2604f4"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "# Закодируем в Bag of N-grams: от 1 до 3 -грамм \n",
    "train_encoded, train_vectorizer = vectorize_as_bongrams(train_data['prep_text'].to_list())\n",
    "test_encoded = train_vectorizer.transform(test_data['prep_text'].to_list()).toarray() "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T18:30:54.494335600Z",
     "start_time": "2023-11-25T18:30:54.294470300Z"
    }
   },
   "id": "e3fe948d45354d8b"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 12000)\n"
     ]
    },
    {
     "data": {
      "text/plain": "            0    1    2    3    4    5    6    7    8    9    ...  293  294  \\\nable          0    0    0    0    0    0    0    0    0    0  ...    0    0   \nable to       0    0    0    0    0    0    0    0    0    0  ...    0    0   \nable to do    0    0    0    0    0    0    0    0    0    0  ...    0    0   \nabot          0    0    0    0    0    0    0    0    0    0  ...    0    0   \nabout         1    1    0    1    1    2    4    4    2    1  ...    1    0   \n\n            295  296  297  298  299  300  301  302  \nable          0    0    0    0    0    0    0    0  \nable to       0    0    0    0    0    0    0    0  \nable to do    0    0    0    0    0    0    0    0  \nabot          0    0    0    0    0    0    0    0  \nabout         1    1    1    2    2    2    2    2  \n\n[5 rows x 303 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n      <th>300</th>\n      <th>301</th>\n      <th>302</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>able</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>able to</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>able to do</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>abot</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>about</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 303 columns</p>\n</div>"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Рассмотрим первые закодированные с помощью Bag of words слова \n",
    "print(train_encoded.shape)\n",
    "pd.DataFrame(train_encoded.transpose(), index=train_vectorizer.get_feature_names_out()).head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T18:30:55.478336700Z",
     "start_time": "2023-11-25T18:30:55.440278200Z"
    }
   },
   "id": "a43cdb9730aa5731"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "def get_tfidf_matrix(texts: list[str], ngram_min=1, ngram_max=3, max_words=3000):\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range = (ngram_min, ngram_max), max_features = max_words)\n",
    "    emb = tfidf_vectorizer.fit_transform(texts).toarray()\n",
    "    return emb, tfidf_vectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T18:31:00.180250900Z",
     "start_time": "2023-11-25T18:31:00.150704800Z"
    }
   },
   "id": "38fe56b4a9c6571a"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "# Закодируем слова с помощью tf-idf \n",
    "train_tfidf, train_tfidf_vectorizer = get_tfidf_matrix(train_data['prep_text'].to_list())\n",
    "test_tfidf = train_tfidf_vectorizer.transform(test_data['prep_text'].to_list()).toarray() "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T18:31:01.548831300Z",
     "start_time": "2023-11-25T18:31:01.339779300Z"
    }
   },
   "id": "e165752bfcacd917"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "                           0         1    2         3         4         5    \\\nable                  0.000000  0.000000  0.0  0.000000  0.000000  0.000000   \nable to               0.000000  0.000000  0.0  0.000000  0.000000  0.000000   \nabout                 0.025747  0.025093  0.0  0.025582  0.023507  0.042626   \nabout arguments       0.000000  0.000000  0.0  0.000000  0.000000  0.000000   \nabout arguments with  0.000000  0.000000  0.0  0.000000  0.000000  0.000000   \n\n                           6         7         8         9    ...       293  \\\nable                  0.000000  0.000000  0.000000  0.000000  ...  0.000000   \nable to               0.000000  0.000000  0.000000  0.000000  ...  0.000000   \nabout                 0.085678  0.086063  0.035413  0.020578  ...  0.021694   \nabout arguments       0.000000  0.000000  0.000000  0.000000  ...  0.000000   \nabout arguments with  0.000000  0.000000  0.000000  0.000000  ...  0.000000   \n\n                      294       295       296       297       298       299  \\\nable                  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \nable to               0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \nabout                 0.0  0.019373  0.023854  0.020672  0.041005  0.043803   \nabout arguments       0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \nabout arguments with  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n\n                           300       301       302  \nable                  0.000000  0.000000  0.000000  \nable to               0.000000  0.000000  0.000000  \nabout                 0.045676  0.053066  0.044421  \nabout arguments       0.000000  0.000000  0.000000  \nabout arguments with  0.000000  0.000000  0.000000  \n\n[5 rows x 303 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n      <th>300</th>\n      <th>301</th>\n      <th>302</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>able</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>able to</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>about</th>\n      <td>0.025747</td>\n      <td>0.025093</td>\n      <td>0.0</td>\n      <td>0.025582</td>\n      <td>0.023507</td>\n      <td>0.042626</td>\n      <td>0.085678</td>\n      <td>0.086063</td>\n      <td>0.035413</td>\n      <td>0.020578</td>\n      <td>...</td>\n      <td>0.021694</td>\n      <td>0.0</td>\n      <td>0.019373</td>\n      <td>0.023854</td>\n      <td>0.020672</td>\n      <td>0.041005</td>\n      <td>0.043803</td>\n      <td>0.045676</td>\n      <td>0.053066</td>\n      <td>0.044421</td>\n    </tr>\n    <tr>\n      <th>about arguments</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>about arguments with</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 303 columns</p>\n</div>"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_tfidf.transpose(), index=train_tfidf_vectorizer.get_feature_names_out()).head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T18:31:51.176087800Z",
     "start_time": "2023-11-25T18:31:51.126065400Z"
    }
   },
   "id": "8a285c85aef1b2a2"
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "     index                                           Question Relevant word\n0        2  …I am so happy that summer has come and we are...         visit\n1        3  …I am learning to cook from my mother now. But...          cook\n2        4  …I am so happy that summer has come and we are...     countries\n3        5  …I’ve recently been involved in a school surve...       this is\n4        6  …All of my friends think camping is a perfect ...         watch\n..     ...                                                ...           ...\n298    533  ... Have I ever told you that my dad’s hobby i...       extreme\n299    551                questions about the film in Spanish          film\n300    569                                                          trip to\n301    572                                                             trip\n302    574                                                         shopping\n\n[303 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>Question</th>\n      <th>Relevant word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>…I am so happy that summer has come and we are...</td>\n      <td>visit</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>…I am learning to cook from my mother now. But...</td>\n      <td>cook</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>…I am so happy that summer has come and we are...</td>\n      <td>countries</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>…I’ve recently been involved in a school surve...</td>\n      <td>this is</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>…All of my friends think camping is a perfect ...</td>\n      <td>watch</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>533</td>\n      <td>... Have I ever told you that my dad’s hobby i...</td>\n      <td>extreme</td>\n    </tr>\n    <tr>\n      <th>299</th>\n      <td>551</td>\n      <td>questions about the film in Spanish</td>\n      <td>film</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>569</td>\n      <td></td>\n      <td>trip to</td>\n    </tr>\n    <tr>\n      <th>301</th>\n      <td>572</td>\n      <td></td>\n      <td>trip</td>\n    </tr>\n    <tr>\n      <th>302</th>\n      <td>574</td>\n      <td></td>\n      <td>shopping</td>\n    </tr>\n  </tbody>\n</table>\n<p>303 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = train_tfidf_vectorizer.get_feature_names_out()\n",
    "pd.DataFrame(pd.concat([train_data['Question'].reset_index(), pd.DataFrame(feature_names[np.argmax(train_tfidf, axis=1)], columns=['Relevant word'])], axis=1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T18:46:43.720605300Z",
     "start_time": "2023-11-25T18:46:43.690399200Z"
    }
   },
   "id": "48f189c1aaee2ccb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
